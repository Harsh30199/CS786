
# Implementation of Anderson's venerable "rational" model of categorization.
# Assumes that stimuli were generated by a mixture of Gaussian distributions;
# rather than compute the full Bayesian posterior, it views items sequentially
# and assigns each to the maximum a posteriori cluster.
#
# At the end it is presented with a stimulus with one item missing, and
# predicts the probability that its value is a '0' or a '1'.
#
# Implemented in python by John McDonnell
#
# References: Anderson (1990) and Anderson (1991),

import numpy as np
from random import shuffle
import pandas as pd

#Utility functions:

class dLocalMAP:
    """
    See Anderson (1990, 1991)
    'Categories' renamed 'clusters' to avoid confusion.
    Discrete version.
    
    Stimulus format is a list of integers from 0 to n-1 where n is the number
    of possible features (e.g. [1,0,1])
    
    args: c, alphas
    """
    
    def __init__(self, args):
        self.partition = [[]]
        self.c, self.alpha = args
        ##print('Alpha old',self.alpha.shape)
        self.alpha0 = [3,3,3]
        self.N = 0
        ###print('C',self.c)
        ##print('Alpha', self.alpha0)
    def probClustVal(self, k, i, val):
        """Find P(j|k)"""
        cj = len([x for x in self.partition[k] if x[i]==val])
        nk = len(self.partition[k])
        return (cj + self.alpha[i][val])/(nk + self.alpha0[i])
    
    def condclusterprob(self, stim, k):
        """Find P(F|k)"""
        pjks = []
        
        for i in range(len(stim)):

            cj = len([x for x in self.partition[k] if x[i]==stim[i]])

            nk = len(self.partition[k])
            ##print(cj,i,stim[i],self.alpha.shape)
            pjks.append((cj + self.alpha[i][stim[i]])/(nk + self.alpha0[i]) )
        return np.product( pjks )
        
    
    def posterior(self, stim):
        """Find P(k|F) for each cluster"""
        pk = np.zeros( len(self.partition) )
        pFk = np.zeros( len(self.partition) )
        
        # existing clusters:
        for k in range(len(self.partition)):
            pk[k] = self.c * len(self.partition[k])/ ((1-self.c) + self.c * self.N)
            if len(self.partition[k])==0: # case of new cluster
                pk[k] = (1-self.c) / (( 1-self.c ) + self.c * self.N)
            pFk[k] = self.condclusterprob( stim, k)
        
        # put it together
        pkF = (pk*pFk) # / sum( pk*pFk )
##        if stim[0] == 92:
##            print(pkF)
        return pkF
    
    def stimulate(self, stim):
        """Argmax of P(k|F) + P(0|F)"""
        winner = np.argmax( self.posterior(stim) )
##        print ("Stim: ", stim)
##        print ("Partition: ", self.partition)
##        print (self.posterior(stim))
        
        if len(self.partition[winner]) == 0:
            self.partition.append( [] )
        self.partition[winner].append(stim)
        
        self.N += 1
    
    def query(self, stimulus):
        """Queried value should be -1."""
        qdim = -1
        for i in range(len(stimulus)):
            if stimulus[i] < 0:
                if qdim != -1:
                    raise (Exception, "ERROR: Multiple dimensions queried.")
                qdim = i
        
        self.N = sum([len(x) for x in self.partition])
        
        pkF = self.posterior(stimulus)
        pkF = pkF[:-1] / sum(pkF[:-1]) # eliminate `new cluster' prob
        
        pjF = np.array( [sum( [ pkF[k] * self.probClustVal(k, qdim, j) \
                for k in range(len(self.partition)-1)] ) 
                for j in range(len( self.alpha[qdim] ))] )
        
        return pjF / sum(pjF)

def testlocalmapD():
    """
    Tests the Anderson's ratinal model using the Medin & Schaffer (1978) data.
    
    This script will print out the probability that each item belongs to each
    of the existing clusters or to a new cluster, and the model assign it to
    the most likely cluster. To see that the model is working correctly, you
    can follow along with Anderson (1991), which steps through in the same way.
    """
    for har in range(10):
        res =[]
        np.random.seed(har)
        for i in range(10):
            model = dLocalMAP([0.33, np.ones((3,3))])
            
            
            df = pd.read_csv('X.csv',header = None)
            train = np.array(df)
            np.random.shuffle(train)
            df = pd.read_csv('y.csv',header = None)
            df[2] = -1
            test = np.array(df)
            np.random.shuffle(test)

         
            for t in train:
                t[0] = ( t[0]-34)//20
                t[1] = (t[1] -55)//10
                t[2] =  t[2]-1
                
                model.stimulate(t)

            

            temp = [0]*3
            for q in test:
                
                t[0] = (q[0]-34)//20
                t[1] = (q[1] -55)//10
                t[2] = q[2]
                t[2] = np.argmax(model.query(t))
                model.stimulate(t)
                q[2] = t[2]+1
            ##print('Classification for ',i+1,'th shuffle',sep='',end='\n\n')
            df = pd.DataFrame(test,columns = ['Weight' ,'Height',  'Label' ])
            ##print(df.head(10),end='\n\n\n')
            res.append(sorted(test, key=lambda x: x[0]))

        check(res)

def check(res):
  final_res = []
  for i in range(10):
    temp = list(res[0][i])+[res[1][i][2]]+[res[2][i][2]]+[res[3][i][2]]+[res[4][i][2]]+[res[5][i][2]]+[res[6][i][2]]+[res[7][i][2]]+[res[8][i][2]]+[res[9][i][2]]
    final_res.append(temp)

  print('Classification for all shuffles ',end='\n\n')
  df = pd.DataFrame(final_res,columns = ['Weight' ,'Height',  'Shuffle 1' , 'Shuffle 2',  'Shuffle 3',  'Shuffle 4','Shuffle 5','6','7','8','9','10'])
  print(df.head(10))

def main():
    testlocalmapD()

if __name__ == '__main__':
    main()
