{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:40.598072Z",
     "start_time": "2022-04-30T17:56:39.240400Z"
    },
    "id": "-TQi-ypcPldC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwuOE2pk00a6"
   },
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qao4MqzOURev"
   },
   "source": [
    "#### When $\\alpha = [0.65,0.35]  $ for Weight and Height respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:40.613033Z",
     "start_time": "2022-04-30T17:56:40.602062Z"
    },
    "id": "oChXkpf0hQjP"
   },
   "outputs": [],
   "source": [
    "def sim(x,y):\n",
    "  dist = np.abs(0.65*(x[0]-y[0]) + 0.35*(x[1]-y[1]))         ## Weight is more likely to be used for categorization\n",
    "  return np.exp(-1*dist)     ##Beta is set to 1 as suggsted in paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:40.628988Z",
     "start_time": "2022-04-30T17:56:40.616024Z"
    },
    "id": "iGqwafbci9Yi"
   },
   "outputs": [],
   "source": [
    "##Calculates N(R,x) for each exeemplar x\n",
    "\n",
    "def calcN(tl):\n",
    "\n",
    "  N = np.zeros((len(tl),3))\n",
    "  for i in range(len(tl)):\n",
    "    N[i][tl[i]-1]+=1\n",
    "  \n",
    "  return N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTIaTGjKw-Nb"
   },
   "source": [
    "#### When $\\gamma = [0.34,0.34,0.32]$ for Categories Small, Average , Large respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:40.644042Z",
     "start_time": "2022-04-30T17:56:40.631985Z"
    },
    "id": "OQy3wc4thJ6Y"
   },
   "outputs": [],
   "source": [
    "def predict(train,y,N):\n",
    "  votes = [0.0]*3\n",
    "  gamma = [0.34,0.34,0.32] ##Prior probability of new stimilus being in category c\n",
    "\n",
    "  for i in range(3):\n",
    "    for j in range(len(train)):\n",
    "      votes[i] += N[j][i]*sim(train[j][0:2],y) ##For each exemplar calculate similarity with new stimulus and take product with exemplar's membership count in that category\n",
    "    \n",
    "    votes[i]*= gamma[i] ## Multiplying with prior\n",
    "\n",
    "  votes = votes/np.sum(votes)\n",
    "\n",
    "  return np.argmax(votes)+1 ##Return category with highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:40.689922Z",
     "start_time": "2022-04-30T17:56:40.646038Z"
    },
    "id": "63hh9qVCo2Vi"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('X.csv',header = None) ##Reads Training Dataset\n",
    "train = np.array(df)\n",
    "df = pd.read_csv('y.csv',header = None) ##Reads Test Dataset\n",
    "df[2] = -1 ## Initializes category of all stimulus to -1\n",
    "test = np.array(df)\n",
    "\n",
    "\n",
    "for t in range(len(test)):\n",
    "  N = calcN(train[:,-1]) ##Calculates N(R,x)\n",
    "  test[t][2] = predict(train,test[t][0:2],N) ##Store Category\n",
    "  \n",
    "  train = np.concatenate((train,test[t].reshape(1,3)),axis = 0) ## Add this test stimulus to training data for succeeding test sstimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:40.720877Z",
     "start_time": "2022-04-30T17:56:40.691914Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z4UMpN-Wqa-H",
    "outputId": "48abd7bc-9d33-4407-9b24-ec6ee9cd6e42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Weight  Height  Label\n",
      "0      74      67      2\n",
      "1      69      63      2\n",
      "2      92      81      3\n",
      "3      64      61      2\n",
      "4      66      84      3\n",
      "5      76      68      3\n",
      "6      61      58      2\n",
      "7      64      76      2\n",
      "8      68      66      2\n",
      "9      34      61      1\n"
     ]
    }
   ],
   "source": [
    "## Displays and stores predicted category labels\n",
    "pd.DataFrame(test).to_csv(\"test_gcm_a1g1.csv\",header=False,index=False)\n",
    "print(pd.DataFrame(test,columns = ['Weight' ,'Height',  'Label' ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDH0XKx-xY6R"
   },
   "source": [
    "#### When $\\gamma = [0.34,0.49,0.17]$ for Categories Small, Average , Large respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:40.736796Z",
     "start_time": "2022-04-30T17:56:40.724829Z"
    },
    "id": "3Sp-6mqgxY6T"
   },
   "outputs": [],
   "source": [
    "def predict(train,y,N):\n",
    "  votes = [0.0]*3\n",
    "  gamma = [0.34,0.49,0.17]\n",
    "  for i in range(3):\n",
    "    for j in range(len(train)):\n",
    "      votes[i] += N[j][i]*sim(train[j][0:2],y)\n",
    "    \n",
    "    votes[i]*= gamma[i]\n",
    "\n",
    "  votes = votes/np.sum(votes)\n",
    "\n",
    "  return np.argmax(votes)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:40.798677Z",
     "start_time": "2022-04-30T17:56:40.743776Z"
    },
    "id": "BRf8hTpYxY6Y"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('X.csv',header = None)\n",
    "train = np.array(df)\n",
    "df = pd.read_csv('y.csv',header = None)\n",
    "df[2] = 0\n",
    "test = np.array(df)\n",
    "\n",
    "\n",
    "for t in range(len(test)):\n",
    "  N = calcN(train[:,-1])\n",
    "  test[t][2] = predict(train,test[t][0:2],N)\n",
    " \n",
    "  train = np.concatenate((train,test[t].reshape(1,3)),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:40.813679Z",
     "start_time": "2022-04-30T17:56:40.803665Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pK-GttLmxm2K",
    "outputId": "554de011-9caa-41b3-babe-eef9e56b611f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Weight  Height  Label\n",
      "0      74      67      2\n",
      "1      69      63      2\n",
      "2      92      81      3\n",
      "3      64      61      2\n",
      "4      66      84      2\n",
      "5      76      68      3\n",
      "6      61      58      2\n",
      "7      64      76      2\n",
      "8      68      66      2\n",
      "9      34      61      1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pd.DataFrame(test).to_csv(\"test_gcm_a1g2.csv\",header=False,index=False)\n",
    "print(pd.DataFrame(test,columns = ['Weight' ,'Height',  'Label' ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2FslAU8U5Dh"
   },
   "source": [
    "#### When $\\alpha = [0.55,0.45]  $ for Weight and Height respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:40.843577Z",
     "start_time": "2022-04-30T17:56:40.816645Z"
    },
    "id": "0LE8T7qqTBC_"
   },
   "outputs": [],
   "source": [
    "def sim2(x,y):\n",
    "  dist = np.abs(0.55*(x[0]-y[0]) + 0.45*(x[1]-y[1]))         ## Weight is more likely to be used for categorization\n",
    "  return np.exp(-1*dist)     ##Beta is set to 1 as suggsted in paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa2EPbUpTBDD"
   },
   "source": [
    "#### When $\\gamma = [0.34,0.34,0.32]$ for Categories Small, Average , Large respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:40.859493Z",
     "start_time": "2022-04-30T17:56:40.845531Z"
    },
    "id": "LUB6lBp8TBDE"
   },
   "outputs": [],
   "source": [
    "def predict(train,y,N):\n",
    "  votes = [0.0]*3\n",
    "  gamma = [0.34,0.34,0.32]\n",
    "  for i in range(3):\n",
    "    for j in range(len(train)):\n",
    "      votes[i] += N[j][i]*sim2(train[j][0:2],y)\n",
    "    \n",
    "    votes[i]*= gamma[i]\n",
    "\n",
    "  votes = votes/np.sum(votes)\n",
    "\n",
    "  return np.argmax(votes)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:40.922392Z",
     "start_time": "2022-04-30T17:56:40.863483Z"
    },
    "id": "i2WhCCKNTBDG"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('X.csv',header = None)\n",
    "train = np.array(df)\n",
    "df = pd.read_csv('y.csv',header = None)\n",
    "df[2] = -1\n",
    "test = np.array(df)\n",
    "\n",
    "\n",
    "for t in range(len(test)):\n",
    "  N = calcN(train[:,-1])\n",
    "  test[t][2] = predict(train,test[t][0:2],N)\n",
    "  \n",
    "  train = np.concatenate((train,test[t].reshape(1,3)),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:40.953227Z",
     "start_time": "2022-04-30T17:56:40.926378Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LsNZReRfTBDH",
    "outputId": "6230922f-1a68-434c-bd51-9da22a346eaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Weight  Height  Label\n",
      "0      74      67      2\n",
      "1      69      63      2\n",
      "2      92      81      3\n",
      "3      64      61      2\n",
      "4      66      84      3\n",
      "5      76      68      3\n",
      "6      61      58      1\n",
      "7      64      76      2\n",
      "8      68      66      2\n",
      "9      34      61      1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pd.DataFrame(test).to_csv(\"test_gcm_a2g1.csv\",header=False,index=False)\n",
    "print(pd.DataFrame(test,columns = ['Weight' ,'Height',  'Label' ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJN-ON8fTBDI"
   },
   "source": [
    "#### When $\\gamma = [0.34,0.49,0.17]$ for Categories Small, Average , Large respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:40.969137Z",
     "start_time": "2022-04-30T17:56:40.954213Z"
    },
    "id": "56QB_FW9TBDJ"
   },
   "outputs": [],
   "source": [
    "def predict(train,y,N):\n",
    "  votes = [0.0]*3\n",
    "  gamma = [0.34,0.49,0.17]\n",
    "  for i in range(3):\n",
    "    for j in range(len(train)):\n",
    "      votes[i] += N[j][i]*sim2(train[j][0:2],y)\n",
    "    \n",
    "    votes[i]*= gamma[i]\n",
    "\n",
    "  votes = votes/np.sum(votes)\n",
    "\n",
    "  return np.argmax(votes)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:41.031068Z",
     "start_time": "2022-04-30T17:56:40.972126Z"
    },
    "id": "MpSMuqXkTBDK"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('X.csv',header = None)\n",
    "train = np.array(df)\n",
    "df = pd.read_csv('y.csv',header = None)\n",
    "df[2] = 0\n",
    "test = np.array(df)\n",
    "\n",
    "\n",
    "for t in range(len(test)):\n",
    "  N = calcN(train[:,-1])\n",
    "  test[t][2] = predict(train,test[t][0:2],N)\n",
    "  \n",
    "  train = np.concatenate((train,test[t].reshape(1,3)),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:41.047061Z",
     "start_time": "2022-04-30T17:56:41.033067Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "go4ZF5UETBDL",
    "outputId": "13045828-57b6-4e2f-89db-e7f2cebea4b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Weight  Height  Label\n",
      "0      74      67      2\n",
      "1      69      63      2\n",
      "2      92      81      3\n",
      "3      64      61      2\n",
      "4      66      84      3\n",
      "5      76      68      3\n",
      "6      61      58      2\n",
      "7      64      76      2\n",
      "8      68      66      2\n",
      "9      34      61      1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pd.DataFrame(test).to_csv(\"test_gcm_a2g2.csv\",header=False,index=False)\n",
    "print(pd.DataFrame(test,columns = ['Weight' ,'Height',  'Label' ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtBnFyIS05c9"
   },
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:41.092907Z",
     "start_time": "2022-04-30T17:56:41.050017Z"
    },
    "id": "bU-6VzJZ07V4"
   },
   "outputs": [],
   "source": [
    "class dLocalMAP:\n",
    "    \"\"\"\n",
    "    See Anderson (1990, 1991)\n",
    "    'Categories' renamed 'clusters' to avoid confusion.\n",
    "    Discrete version.\n",
    "    \n",
    "    Stimulus format is a list of integers from 0 to n-1 where n is the number\n",
    "    of possible features (e.g. [1,0,1])\n",
    "    \n",
    "    args: c, alphas\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, args): ##Parameter Initialization\n",
    "        self.partition = [[]]\n",
    "        self.c, self.alpha = args\n",
    "        ## c is coupling probability and alpha is prior probability of value j occuring on ith dimension\n",
    "        self.alpha0 = [sum(self.alpha[0]),sum(self.alpha[1]),sum(self.alpha[2])] ## Sum of alpha values for each dimension\n",
    "        self.N = 0   ##Total no of stimulus experienced\n",
    "        \n",
    "    def probClustVal(self, k, i, val):\n",
    "        \"\"\"Find P(j|k)\"\"\"\n",
    "        ## No of seen stimulus in cluster k with same value at ith dimension as test stimulus\n",
    "        cj = len([x for x in self.partition[k] if x[i]==val+1]) ##Modified Code Here\n",
    "        ## Total no of stimulus in cluster k\n",
    "        nk = len(self.partition[k]) ## Modified Code Here\n",
    "        \n",
    "        return (cj + self.alpha[i][val])/(nk + self.alpha0[i]) ## Probability of showing value val on ith dimension if from cluster k\n",
    "    \n",
    "    def condclusterprob(self, stim, k): ##Calculates probability of seeing feature F assuming stimulus is from cluster k\n",
    "        \"\"\"Find P(F|k)\"\"\"\n",
    "        pjks = []\n",
    "        \n",
    "        for i in range(len(stim)): ## For each dimension\n",
    "            ## Calculates no of seen stimuli with near value as new stimuli in cluster k\n",
    "            cj = len([x for x in self.partition[k] if x[i] in range(stim[i]-2,stim[i]+3)]) ##Modified Code here\n",
    "            \n",
    "            nk = len(self.partition[k]) ## No of seen stimuli in cluster k\n",
    "            \n",
    "            pjks.append((cj + self.alpha[i][stim[i]-1])/(nk + self.alpha0[i]) ) ## Stores Conditional Probability for each dimension\n",
    "        \n",
    "        return np.product( pjks ) ## Returns conditional probability\n",
    "        \n",
    "    \n",
    "    def posterior(self, stim): ## Calculates probability of stimulus belonging to cluster k given observed features\n",
    "        \"\"\"Find P(k|F) for each cluster\"\"\"\n",
    "        pk = np.zeros( len(self.partition) )\n",
    "        pFk = np.zeros( len(self.partition) )\n",
    "        \n",
    "        # existing clusters:\n",
    "        for k in range(len(self.partition)):\n",
    "            pk[k] = self.c * len(self.partition[k])/ ((1-self.c) + self.c * self.N) ## Calculates probability of it being from previous clusters\n",
    "            if len(self.partition[k])==0: # case of new cluster\n",
    "                pk[k] = (1-self.c) / (( 1-self.c ) + self.c * self.N) ## Calculates probability that it belongs to new cluster\n",
    "            \n",
    "            pFk[k] = self.condclusterprob( stim, k) ## Calculates Conditional Probability\n",
    "        \n",
    "        # put it together\n",
    "        pkF = (pk*pFk) # / sum( pk*pFk ) Multiply Prior and Conditional Probability\n",
    "\n",
    "        return pkF\n",
    "    \n",
    "    def stimulate(self, stim):\n",
    "        \"\"\"Argmax of P(k|F) + P(0|F)\"\"\"\n",
    "        winner = np.argmax( self.posterior(stim) ) ##Assigns training stimulus to a cluster based on posterior value\n",
    "\n",
    "        \n",
    "        if len(self.partition[winner]) == 0: ##if assigned to new cluster then add a new empty cluster and assign stimulus to winner cluster\n",
    "            self.partition.append( [] )\n",
    "        self.partition[winner].append(stim)\n",
    "        \n",
    "        self.N += 1 ##Increments counter by 1 for seen stimulus\n",
    "    \n",
    "    def query(self, stimulus):\n",
    "        \"\"\"Queried value should be -1.\"\"\"\n",
    "        qdim = -1\n",
    "        ## FInds out which dimension value needs to be predicted and enforces that there is only one such dimension\n",
    "        for i in range(len(stimulus)):\n",
    "            if stimulus[i] < 0:\n",
    "                if qdim != -1:\n",
    "                    raise (Exception, \"ERROR: Multiple dimensions queried.\")\n",
    "                qdim = i\n",
    "        \n",
    "        self.N = sum([len(x) for x in self.partition]) ##Calculates no of exemplars seen\n",
    "        \n",
    "        pkF = self.posterior(stimulus) ## Calculates posterior of test stimulus\n",
    "        pkF = pkF[:-1] / sum(pkF[:-1]) # eliminate `new cluster' prob\n",
    "        \n",
    "        pjF = np.array( [sum( [ pkF[k] * self.probClustVal(k, qdim, j) \\\n",
    "                for k in range(len(self.partition)-1)] ) \n",
    "                for j in range(len( self.alpha[qdim] ))] )\n",
    "        \n",
    "        return pjF / sum(pjF) ## Calculates and return probability of a value occuring for queried dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypxVFE1x3h5b"
   },
   "source": [
    "#### When $c = 0.0001 $ where $c$ is Coupling Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C59v2YYe39yR"
   },
   "source": [
    "#### When $ \\alpha = [0.21,0.27 , 0.52] $ for Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:41.123822Z",
     "start_time": "2022-04-30T17:56:41.096894Z"
    },
    "id": "UpyFaVQh1WPf"
   },
   "outputs": [],
   "source": [
    "def testlocalmapD():\n",
    "    \"\"\"\n",
    "    Tests the Anderson's ratinal model using the Medin & Schaffer (1978) data.\n",
    "    \n",
    "    This script will print out the probability that each item belongs to each\n",
    "    of the existing clusters or to a new cluster, and the model assign it to\n",
    "    the most likely cluster. To see that the model is working correctly, you\n",
    "    can follow along with Anderson (1991), which steps through in the same way.\n",
    "    \"\"\"\n",
    " \n",
    "    for i in range(1):\n",
    "        model = dLocalMAP([0.0001, np.array([np.ones(100)/100,np.ones(100)/100,[0.21,0.27,0.52]])]) ## Initializes c and alpha\n",
    "        \n",
    "        \n",
    "        df = pd.read_csv('X.csv',header = None) ##Reads training data\n",
    "        train = np.array(df) ##Converts into numpy array\n",
    "\n",
    "        df = pd.read_csv('y.csv',header = None) ##Reads test data\n",
    "        df[2] = -1 ## Initializes category label to -1 \n",
    "        test = np.array(df) ##Converts into numpy array\n",
    "\n",
    "\n",
    "      \n",
    "        for t in train: ## Add each training stimulus to RMC model\n",
    "            model.stimulate(t)\n",
    "\n",
    "        for q in test: ## For each test stimulus predict category label\n",
    "\n",
    "            q[2] = np.argmax(model.query(q))+1\n",
    "            model.stimulate(q) ## Add current test stimulus as training stimulus for next stimulus\n",
    "            \n",
    "    return test\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:41.276666Z",
     "start_time": "2022-04-30T17:56:41.125816Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XFcZMYPL1u65",
    "outputId": "35ae9de5-16cb-4a5f-c61a-e06e21ba8bd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Weight  Height  Label\n",
      "0      74      67      3\n",
      "1      69      63      2\n",
      "2      92      81      3\n",
      "3      64      61      2\n",
      "4      66      84      2\n",
      "5      76      68      3\n",
      "6      61      58      1\n",
      "7      64      76      2\n",
      "8      68      66      2\n",
      "9      34      61      1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = testlocalmapD()\n",
    "df = pd.DataFrame(test,columns = ['Weight' ,'Height',  'Label' ])\n",
    "print(df.head(10),end='\\n\\n\\n')\n",
    "df.to_csv(\"test_rmc_a1c1.csv\",header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZ6E6BvQ4Mz4"
   },
   "source": [
    "#### When $ \\alpha = [0.33,0.33 , 0.34] $ for Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:41.292000Z",
     "start_time": "2022-04-30T17:56:41.278701Z"
    },
    "id": "8rzN-Drj3bl_"
   },
   "outputs": [],
   "source": [
    "def testlocalmapD():\n",
    "    \"\"\"\n",
    "    Tests the Anderson's ratinal model using the Medin & Schaffer (1978) data.\n",
    "    \n",
    "    This script will print out the probability that each item belongs to each\n",
    "    of the existing clusters or to a new cluster, and the model assign it to\n",
    "    the most likely cluster. To see that the model is working correctly, you\n",
    "    can follow along with Anderson (1991), which steps through in the same way.\n",
    "    \"\"\"\n",
    " \n",
    "    for i in range(1):\n",
    "        model = dLocalMAP([0.0001, np.array([np.ones(100)/100,np.ones(100)/100,[0.33,0.33,0.34]])])\n",
    "        \n",
    "        \n",
    "        df = pd.read_csv('X.csv',header = None)\n",
    "        train = np.array(df)\n",
    "\n",
    "        df = pd.read_csv('y.csv',header = None)\n",
    "        df[2] = -1\n",
    "        test = np.array(df)\n",
    " \n",
    "\n",
    "      \n",
    "        for t in train:\n",
    "            model.stimulate(t)\n",
    "\n",
    "        for q in test:\n",
    "  \n",
    "            q[2] = np.argmax(model.query(q))+1\n",
    "            model.stimulate(q)\n",
    "            \n",
    "    return test\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:41.445857Z",
     "start_time": "2022-04-30T17:56:41.294030Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uX0R23eT3bmA",
    "outputId": "a46e32eb-520a-4966-9423-63be8b534859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Weight  Height  Label\n",
      "0      74      67      3\n",
      "1      69      63      2\n",
      "2      92      81      2\n",
      "3      64      61      2\n",
      "4      66      84      2\n",
      "5      76      68      3\n",
      "6      61      58      1\n",
      "7      64      76      2\n",
      "8      68      66      2\n",
      "9      34      61      1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = testlocalmapD()\n",
    "df = pd.DataFrame(test,columns = ['Weight' ,'Height',  'Label' ])\n",
    "print(df.head(10),end='\\n\\n\\n')\n",
    "df.to_csv(\"test_rmc_a1c2.csv\",header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-TiBNWU4SAP"
   },
   "source": [
    "#### When $ c = 0.1 $ where $c$ is Coupling Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIA5EUpc4fmm"
   },
   "source": [
    "#### When $ \\alpha = [0.21,0.27 , 0.52] $ for Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:41.460780Z",
     "start_time": "2022-04-30T17:56:41.447813Z"
    },
    "id": "KO3uGlEx4fmn"
   },
   "outputs": [],
   "source": [
    "def testlocalmapD():\n",
    "    \"\"\"\n",
    "    Tests the Anderson's ratinal model using the Medin & Schaffer (1978) data.\n",
    "    \n",
    "    This script will print out the probability that each item belongs to each\n",
    "    of the existing clusters or to a new cluster, and the model assign it to\n",
    "    the most likely cluster. To see that the model is working correctly, you\n",
    "    can follow along with Anderson (1991), which steps through in the same way.\n",
    "    \"\"\"\n",
    " \n",
    "    for i in range(1):\n",
    "        model = dLocalMAP([0.1, np.array([np.ones(100)/100,np.ones(100)/100,[0.21,0.27,0.52]])])\n",
    "        \n",
    "        \n",
    "        df = pd.read_csv('X.csv',header = None)\n",
    "        train = np.array(df)\n",
    "        \n",
    "        df = pd.read_csv('y.csv',header = None)\n",
    "        df[2] = -1\n",
    "        test = np.array(df)\n",
    "        \n",
    "\n",
    "      \n",
    "        for t in train:\n",
    "            model.stimulate(t)\n",
    "\n",
    "        for q in test:\n",
    "\n",
    "            q[2] = np.argmax(model.query(q))+1\n",
    "            model.stimulate(q)\n",
    "            \n",
    "    return test\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:41.537135Z",
     "start_time": "2022-04-30T17:56:41.468757Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xY8h-Rwx4fmp",
    "outputId": "2365a596-ce50-485f-d22e-030a0cff679f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Weight  Height  Label\n",
      "0      74      67      2\n",
      "1      69      63      2\n",
      "2      92      81      1\n",
      "3      64      61      2\n",
      "4      66      84      2\n",
      "5      76      68      2\n",
      "6      61      58      2\n",
      "7      64      76      2\n",
      "8      68      66      2\n",
      "9      34      61      1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = testlocalmapD()\n",
    "df = pd.DataFrame(test,columns = ['Weight' ,'Height',  'Label' ])\n",
    "print(df.head(10),end='\\n\\n\\n')\n",
    "df.to_csv(\"test_rmc_a2c1.csv\",header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1_7rxTw4fmq"
   },
   "source": [
    "#### When $ \\alpha = [0.33,0.33 , 0.34] $ for Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:41.553090Z",
     "start_time": "2022-04-30T17:56:41.541123Z"
    },
    "id": "Z6wLHPZr4fmr"
   },
   "outputs": [],
   "source": [
    "def testlocalmapD():\n",
    "    \"\"\"\n",
    "    Tests the Anderson's ratinal model using the Medin & Schaffer (1978) data.\n",
    "    \n",
    "    This script will print out the probability that each item belongs to each\n",
    "    of the existing clusters or to a new cluster, and the model assign it to\n",
    "    the most likely cluster. To see that the model is working correctly, you\n",
    "    can follow along with Anderson (1991), which steps through in the same way.\n",
    "    \"\"\"\n",
    " \n",
    "    for i in range(1):\n",
    "        model = dLocalMAP([0.1, np.array([np.ones(100)/100,np.ones(100)/100,[0.33,0.33,0.34]])])\n",
    "        \n",
    "        \n",
    "        df = pd.read_csv('X.csv',header = None)\n",
    "        train = np.array(df)\n",
    "        \n",
    "        df = pd.read_csv('y.csv',header = None)\n",
    "        df[2] = -1\n",
    "        test = np.array(df)\n",
    "        \n",
    "\n",
    "      \n",
    "        for t in train:\n",
    "            model.stimulate(t)\n",
    "\n",
    "        for q in test:\n",
    "            q[2] = np.argmax(model.query(q))+1\n",
    "            model.stimulate(q)\n",
    "            \n",
    "    return test\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:41.632191Z",
     "start_time": "2022-04-30T17:56:41.559074Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_D6PP_Pv4fmr",
    "outputId": "21b56f85-d7ea-48bb-d01a-e02493b2d18e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Weight  Height  Label\n",
      "0      74      67      2\n",
      "1      69      63      2\n",
      "2      92      81      1\n",
      "3      64      61      2\n",
      "4      66      84      2\n",
      "5      76      68      2\n",
      "6      61      58      2\n",
      "7      64      76      2\n",
      "8      68      66      2\n",
      "9      34      61      1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = testlocalmapD()\n",
    "df = pd.DataFrame(test,columns = ['Weight' ,'Height',  'Label' ])\n",
    "print(df.head(10),end='\\n\\n\\n')\n",
    "df.to_csv(\"test_rmc_a2c2.csv\",header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnRcPp3r0iZg"
   },
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56emqVbBZi3x"
   },
   "source": [
    "#### For showing Exchangability of data for GCM we will consider $\\alpha = [0.\n",
    "67,0.33]$ and $\\gamma = [0.34,0.39,0.27]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:41.648118Z",
     "start_time": "2022-04-30T17:56:41.634156Z"
    },
    "id": "TL6olQTebNGD"
   },
   "outputs": [],
   "source": [
    "def sim(x,y):\n",
    "  dist = np.abs(0.67*(x[0]-y[0]) + 0.33*(x[1]-y[1]))   ## Weight is more likely to be used for categorization\n",
    "  return np.exp(-1*dist)     ##Beta is set to 1 as suggsted in paper\n",
    "\n",
    "def calcN(tl):\n",
    "  N = np.zeros((len(tl),3))\n",
    "  for i in range(len(tl)):\n",
    "    N[i][tl[i]-1]+=1\n",
    "  \n",
    "  return N\n",
    "\n",
    "def predict(train,y,N):\n",
    "  votes = [0.0]*3\n",
    "  gamma = [0.34,0.39,0.27]\n",
    "  for i in range(3):\n",
    "    for j in range(len(train)):\n",
    "      votes[i] += N[j][i]*sim(train[j][0:2],y)\n",
    "    \n",
    "    votes[i]*= gamma[i]\n",
    "\n",
    "  votes = votes/np.sum(votes)\n",
    "\n",
    "  return np.argmax(votes)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:41.679038Z",
     "start_time": "2022-04-30T17:56:41.652107Z"
    },
    "id": "IoTBMG-AmLIw"
   },
   "outputs": [],
   "source": [
    "def check(res):\n",
    "  final_res = []\n",
    "  for i in range(10):\n",
    "    temp = list(res[0][i])+[res[1][i][2]]+[res[2][i][2]]+[res[3][i][2]]+[res[4][i][2]]+[res[5][i][2]]+[res[6][i][2]]+[res[7][i][2]]+[res[8][i][2]]+[res[9][i][2]]\n",
    "    final_res.append(temp)\n",
    "\n",
    "  print('Classification for all shuffles ',end='\\n\\n')\n",
    "  df = pd.DataFrame(final_res,columns = ['Weight' ,'Height',  'Shuffle 1' , 'Shuffle 2',  'Shuffle 3',  'Shuffle 4','Shuffle 5','Shuffle 6','Shuffle 7','Shuffle 8','Shuffle 9','Shuffle 10'])\n",
    "  print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:42.144339Z",
     "start_time": "2022-04-30T17:56:41.681029Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNtr-cD9Z96B",
    "outputId": "daae76df-eaba-40d0-dea4-9eb2f899b9f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification for 1th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      64      76      2\n",
      "1      61      58      2\n",
      "2      68      66      2\n",
      "3      34      61      1\n",
      "4      92      81      3\n",
      "5      69      63      2\n",
      "6      66      84      2\n",
      "7      76      68      3\n",
      "8      64      61      2\n",
      "9      74      67      2\n",
      "\n",
      "\n",
      "Classification for 2th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      69      63      2\n",
      "1      66      84      2\n",
      "2      76      68      3\n",
      "3      34      61      1\n",
      "4      61      58      2\n",
      "5      64      61      2\n",
      "6      64      76      2\n",
      "7      68      66      2\n",
      "8      74      67      2\n",
      "9      92      81      3\n",
      "\n",
      "\n",
      "Classification for 3th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      66      84      2\n",
      "1      74      67      2\n",
      "2      92      81      3\n",
      "3      68      66      2\n",
      "4      76      68      3\n",
      "5      61      58      2\n",
      "6      69      63      2\n",
      "7      64      76      2\n",
      "8      34      61      1\n",
      "9      64      61      2\n",
      "\n",
      "\n",
      "Classification for 4th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      34      61      1\n",
      "1      64      61      2\n",
      "2      64      76      2\n",
      "3      76      68      3\n",
      "4      66      84      2\n",
      "5      92      81      3\n",
      "6      68      66      2\n",
      "7      61      58      2\n",
      "8      74      67      2\n",
      "9      69      63      2\n",
      "\n",
      "\n",
      "Classification for 5th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      34      61      1\n",
      "1      92      81      3\n",
      "2      66      84      2\n",
      "3      64      61      2\n",
      "4      64      76      2\n",
      "5      76      68      3\n",
      "6      74      67      2\n",
      "7      68      66      2\n",
      "8      61      58      2\n",
      "9      69      63      2\n",
      "\n",
      "\n",
      "Classification for 6th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      76      68      3\n",
      "1      68      66      2\n",
      "2      34      61      1\n",
      "3      61      58      2\n",
      "4      64      76      2\n",
      "5      66      84      2\n",
      "6      74      67      2\n",
      "7      69      63      2\n",
      "8      92      81      3\n",
      "9      64      61      2\n",
      "\n",
      "\n",
      "Classification for 7th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      76      68      3\n",
      "1      64      61      2\n",
      "2      74      67      2\n",
      "3      68      66      2\n",
      "4      34      61      1\n",
      "5      61      58      2\n",
      "6      69      63      2\n",
      "7      66      84      2\n",
      "8      64      76      2\n",
      "9      92      81      3\n",
      "\n",
      "\n",
      "Classification for 8th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      68      66      2\n",
      "1      66      84      2\n",
      "2      69      63      2\n",
      "3      74      67      2\n",
      "4      64      76      2\n",
      "5      92      81      3\n",
      "6      61      58      2\n",
      "7      76      68      3\n",
      "8      64      61      2\n",
      "9      34      61      1\n",
      "\n",
      "\n",
      "Classification for 9th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      66      84      2\n",
      "1      68      66      2\n",
      "2      34      61      1\n",
      "3      64      61      2\n",
      "4      61      58      2\n",
      "5      69      63      2\n",
      "6      92      81      3\n",
      "7      74      67      2\n",
      "8      64      76      2\n",
      "9      76      68      3\n",
      "\n",
      "\n",
      "Classification for 10th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      34      61      1\n",
      "1      61      58      2\n",
      "2      64      76      2\n",
      "3      69      63      2\n",
      "4      64      61      2\n",
      "5      76      68      3\n",
      "6      92      81      3\n",
      "7      68      66      2\n",
      "8      74      67      2\n",
      "9      66      84      2\n",
      "\n",
      "\n",
      "Classification for all shuffles \n",
      "\n",
      "   Weight  Height  Shuffle 1  Shuffle 2  Shuffle 3  Shuffle 4  Shuffle 5  \\\n",
      "0      34      61          1          1          1          1          1   \n",
      "1      61      58          2          2          2          2          2   \n",
      "2      64      76          2          2          2          2          2   \n",
      "3      64      61          2          2          2          2          2   \n",
      "4      66      84          2          2          2          2          2   \n",
      "5      68      66          2          2          2          2          2   \n",
      "6      69      63          2          2          2          2          2   \n",
      "7      74      67          2          2          2          2          2   \n",
      "8      76      68          3          3          3          3          3   \n",
      "9      92      81          3          3          3          3          3   \n",
      "\n",
      "   Shuffle 6  Shuffle 7  Shuffle 8  Shuffle 9  Shuffle 10  \n",
      "0          1          1          1          1           1  \n",
      "1          2          2          2          2           2  \n",
      "2          2          2          2          2           2  \n",
      "3          2          2          2          2           2  \n",
      "4          2          2          2          2           2  \n",
      "5          2          2          2          2           2  \n",
      "6          2          2          2          2           2  \n",
      "7          2          2          2          2           2  \n",
      "8          3          3          3          3           3  \n",
      "9          3          3          3          3           3  \n"
     ]
    }
   ],
   "source": [
    "res =[]\n",
    "np.random.seed(0)\n",
    "for i in range(10):\n",
    "\n",
    "  df = pd.read_csv('X.csv',header = None)\n",
    "  train = np.array(df)\n",
    "  df = pd.read_csv('y.csv',header = None)\n",
    "  df[2] = -1\n",
    "  test = np.array(df)\n",
    "\n",
    "  np.random.shuffle(train)\n",
    "  np.random.shuffle(test)\n",
    "\n",
    "  for t in range(len(test)):\n",
    "    N = calcN(train[:,-1])\n",
    "    test[t][2] = predict(train,test[t][0:2],N)\n",
    "    \n",
    "    train = np.concatenate((train,test[t].reshape(1,3)),axis = 0)\n",
    "  \n",
    "  print('Classification for ',i+1,'th shuffle',sep='',end='\\n\\n')\n",
    "  df = pd.DataFrame(test,columns = ['Weight' ,'Height',  'Label' ])\n",
    "  print(df.head(10),end='\\n\\n\\n')\n",
    "  res.append(sorted(test, key=lambda x: x[0]))\n",
    "\n",
    "check(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFuapaBlc8Kq"
   },
   "source": [
    "#### As we can that even after shuffling training and test data ten times, we get same category labels for test data each time. Therefore GCM has the property of data exchangibility, i.e. the order in which data enters the model does not affect the category labels of the model for any given subset of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFMyOgB12ihA"
   },
   "source": [
    "#### For showing Exchangability of data for RMC we will consider $\\alpha = [0.\n",
    "21,0.27,0.52]$ for category prior and $c = 0.0001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:42.176261Z",
     "start_time": "2022-04-30T17:56:42.147331Z"
    },
    "id": "CJIrPht22AYc"
   },
   "outputs": [],
   "source": [
    "def testlocalmapD2():\n",
    "    \"\"\"\n",
    "    Tests the Anderson's ratinal model using the Medin & Schaffer (1978) data.\n",
    "    \n",
    "    This script will print out the probability that each item belongs to each\n",
    "    of the existing clusters or to a new cluster, and the model assign it to\n",
    "    the most likely cluster. To see that the model is working correctly, you\n",
    "    can follow along with Anderson (1991), which steps through in the same way.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    res =[]\n",
    "    for i in range(10):\n",
    "        model = dLocalMAP([0.0001, np.array([np.ones(100)/100,np.ones(100)/100,[0.21,0.27,0.52]])])\n",
    "        \n",
    "        \n",
    "        df = pd.read_csv('X.csv',header = None)\n",
    "        train = np.array(df)\n",
    "        np.random.shuffle(train)\n",
    "        df = pd.read_csv('y.csv',header = None)\n",
    "        df[2] = -1\n",
    "        test = np.array(df)\n",
    "        np.random.shuffle(test)\n",
    "\n",
    "      \n",
    "        for t in train:\n",
    "            model.stimulate(t)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        for q in test:\n",
    "\n",
    "            q[2] = np.argmax(model.query(q))+1\n",
    "            model.stimulate(q)\n",
    "            \n",
    "        print('Classification for ',i+1,'th shuffle',sep='',end='\\n\\n')\n",
    "        df = pd.DataFrame(test,columns = ['Weight' ,'Height',  'Label' ])\n",
    "        print(df.head(10),end='\\n\\n\\n')\n",
    "        res.append(sorted(test, key=lambda x: x[0]))\n",
    "\n",
    "    check(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:56:43.520322Z",
     "start_time": "2022-04-30T17:56:42.178249Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m8g5c0sO2H2x",
    "outputId": "7ba9c44b-8f62-4eff-aa14-23f1a0299b42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34327392 0.31229831 0.34442777]\n",
      "Classification for 1th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      64      76      2\n",
      "1      61      58      1\n",
      "2      68      66      2\n",
      "3      34      61      1\n",
      "4      92      81      3\n",
      "5      69      63      2\n",
      "6      66      84      2\n",
      "7      76      68      3\n",
      "8      64      61      2\n",
      "9      74      67      3\n",
      "\n",
      "\n",
      "[0.33362286 0.31726823 0.34910891]\n",
      "Classification for 2th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      69      63      2\n",
      "1      66      84      2\n",
      "2      76      68      3\n",
      "3      34      61      1\n",
      "4      61      58      1\n",
      "5      64      61      2\n",
      "6      64      76      2\n",
      "7      68      66      2\n",
      "8      74      67      3\n",
      "9      92      81      3\n",
      "\n",
      "\n",
      "[0.32069293 0.32300951 0.35629755]\n",
      "Classification for 3th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      66      84      2\n",
      "1      74      67      3\n",
      "2      92      81      3\n",
      "3      68      66      2\n",
      "4      76      68      3\n",
      "5      61      58      1\n",
      "6      69      63      2\n",
      "7      64      76      2\n",
      "8      34      61      1\n",
      "9      64      61      2\n",
      "\n",
      "\n",
      "[0.32852 0.32076 0.35072]\n",
      "Classification for 4th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      34      61      1\n",
      "1      64      61      2\n",
      "2      64      76      2\n",
      "3      76      68      3\n",
      "4      66      84      2\n",
      "5      92      81      3\n",
      "6      68      66      2\n",
      "7      61      58      1\n",
      "8      74      67      3\n",
      "9      69      63      2\n",
      "\n",
      "\n",
      "[0.33652138 0.31398575 0.34949287]\n",
      "Classification for 5th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      34      61      1\n",
      "1      92      81      3\n",
      "2      66      84      2\n",
      "3      64      61      2\n",
      "4      64      76      2\n",
      "5      76      68      3\n",
      "6      74      67      3\n",
      "7      68      66      2\n",
      "8      61      58      1\n",
      "9      69      63      2\n",
      "\n",
      "\n",
      "[0.33549002 0.31467332 0.34983666]\n",
      "Classification for 6th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      76      68      3\n",
      "1      68      66      2\n",
      "2      34      61      1\n",
      "3      61      58      1\n",
      "4      64      76      2\n",
      "5      66      84      2\n",
      "6      74      67      3\n",
      "7      69      63      2\n",
      "8      92      81      3\n",
      "9      64      61      2\n",
      "\n",
      "\n",
      "[0.33362286 0.31726823 0.34910891]\n",
      "Classification for 7th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      76      68      3\n",
      "1      64      61      2\n",
      "2      74      67      3\n",
      "3      68      66      2\n",
      "4      34      61      1\n",
      "5      61      58      1\n",
      "6      69      63      2\n",
      "7      66      84      2\n",
      "8      64      76      2\n",
      "9      92      81      3\n",
      "\n",
      "\n",
      "[0.31491736 0.33136364 0.35371901]\n",
      "Classification for 8th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      68      66      2\n",
      "1      66      84      2\n",
      "2      69      63      2\n",
      "3      74      67      3\n",
      "4      64      76      2\n",
      "5      92      81      3\n",
      "6      61      58      1\n",
      "7      76      68      3\n",
      "8      64      61      2\n",
      "9      34      61      1\n",
      "\n",
      "\n",
      "[0.33931734 0.31765683 0.34302583]\n",
      "Classification for 9th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      66      84      2\n",
      "1      68      66      2\n",
      "2      34      61      1\n",
      "3      64      61      2\n",
      "4      61      58      1\n",
      "5      69      63      2\n",
      "6      92      81      3\n",
      "7      74      67      3\n",
      "8      64      76      2\n",
      "9      76      68      3\n",
      "\n",
      "\n",
      "[0.33931734 0.31350554 0.34717712]\n",
      "Classification for 10th shuffle\n",
      "\n",
      "   Weight  Height  Label\n",
      "0      34      61      1\n",
      "1      61      58      1\n",
      "2      64      76      2\n",
      "3      69      63      2\n",
      "4      64      61      2\n",
      "5      76      68      3\n",
      "6      92      81      3\n",
      "7      68      66      2\n",
      "8      74      67      3\n",
      "9      66      84      2\n",
      "\n",
      "\n",
      "Classification for all shuffles \n",
      "\n",
      "   Weight  Height  Shuffle 1  Shuffle 2  Shuffle 3  Shuffle 4  Shuffle 5  \\\n",
      "0      34      61          1          1          1          1          1   \n",
      "1      61      58          1          1          1          1          1   \n",
      "2      64      76          2          2          2          2          2   \n",
      "3      64      61          2          2          2          2          2   \n",
      "4      66      84          2          2          2          2          2   \n",
      "5      68      66          2          2          2          2          2   \n",
      "6      69      63          2          2          2          2          2   \n",
      "7      74      67          3          3          3          3          3   \n",
      "8      76      68          3          3          3          3          3   \n",
      "9      92      81          3          3          3          3          3   \n",
      "\n",
      "   Shuffle 6  Shuffle 7  Shuffle 8  Shuffle 9  Shuffle 10  \n",
      "0          1          1          1          1           1  \n",
      "1          1          1          1          1           1  \n",
      "2          2          2          2          2           2  \n",
      "3          2          2          2          2           2  \n",
      "4          2          2          2          2           2  \n",
      "5          2          2          2          2           2  \n",
      "6          2          2          2          2           2  \n",
      "7          3          3          3          3           3  \n",
      "8          3          3          3          3           3  \n",
      "9          3          3          3          3           3  \n"
     ]
    }
   ],
   "source": [
    "testlocalmapD2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfZZvpCt2MuA"
   },
   "source": [
    "#### As we can that even after shuffling training and test data ten times, we get same category labels for test data each time. Therefore RMC has the property of data exchangibility, i.e. the order in which data enters the model does not affect the category labels of the model for any given subset of data"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CS786 Assignment4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
